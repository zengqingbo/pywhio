---
title: "Python BigData Starter CP01"
cover: "5.jpg"
category: "python"
date: "2019-02-10s"
slug: "python-bigdata-starter-cp01"
tags:
---



# 1、背景介绍



## 1.1 职场新人的困惑



​       当我们走进大学校园的时候，有各种各样专业选择，比如通信工程，自动控制，财务会计，生物工程，和市场营销。一旦选定一个专业，就开始接受系统的专业基础学习与训练，这些学习与训练是如此的完备和科学，让人情不自禁的会感叹在大学的时光中学习了足够多的专业知识，这样足以胜任未来的工作，凭借自己的双手为自己创造一份有尊严的生活。等完成全部的学业，过五关斩六将找的一份满意的工作，进入一个理想的公司或者是工作单位，大家都渴望在新的工作岗位上大展拳脚，早日建功立业。

​      然而现实和想象往往有很大的出入，等上班一周以后，我们会发现很多时间都花费在搜集数据和处理数据上，从多个不同的TXT文本文件中，EXCEL文件中，IT系统中，提取数据，分析数据。 这一类的工作看起来好像没有什么技术含量，但是一旦做起来就感觉非常的麻烦，对人的要求很高。首先需要**眼睛好**，能够长时间的关注一大堆数据，筛选出自己需要的数据，尤其是处理有上百列信息结构的报表，总是感觉电脑屏幕太小了，看起来非常的费劲。 第二对**身体素质好**，遇到一些紧急的分析任务，往往需要在电脑前工作几个小时，搞得腰酸背痛，脖子不舒服，如果身体不好，往往很难胜任。第三就是要**心理素质好**，虽然看起来分析数据，制作报表是一件很简单的事情，而一旦报表生成好了，就关系着部门业绩考核，公司战略决策等重大事宜，不允许出错，必须反复检查，这对一个人心理素质有很高的要求。

​      这时候，你就会发现，虽然在大学的时候接受了大量的专业知识的训练，唯独没有接受过数据分析和报表制作方面的培训，处理一个专项任务，通常有80%的时间花费在提取数据，过滤数据和合并数据上，需要依赖专业知识的分析工作仅仅花费了20%都不到的时间。那么在大学中为什么就没有一门课呢？ 有些新人就想改变这种状况，想通过计算机来解决这个难题，然而绝大多数人都不是计算机科班出生，面对纷繁复杂的编程语言，小插件，数据库，中间件，很难做出选择。等到下定决心开始学习，要经历漫长的学习，实验和调试的工程才会成功。当然这样的幸运儿是少数，大多数人都是在这种困惑中月复一月的从事着繁琐的数据分析工作。





## 1.2几个职场达人

​       虽然大多数人都会有数据分析以及报表制作上遇到麻烦，而也有一些善于学习职场达人能够很快的解决这个难题，享受工作的乐趣，成为人生的赢家。

​       记得2001年的时候，有一位同事因为感觉每月的报表制作工作非常繁琐，就自己动手搭建了一个报表平台，利用EXCEL的VBA作为前台，利用Access数据库为后台，成功实现了日粒度报表的制作和天粒度报表的制作。随着数据规模的不断扩大，ACCESS数据库慢慢开始力不从心，所以就专门打申请购买了一台高性能的台式机，并更换为ORACLE数据库。因为这位同事的出色表现，他后来的发展也非常顺利，2006年被集团总部看中，竞聘到北京总部工作，有了更大的舞台。

​       2014年，我参加了一个数据挖掘软件SPSS的培训，在下午实战环节中，有学员拿出工作中的一个案例请老师现场处理，当时数据源涉及到好几个文件，处理起来非常麻烦，而这位老师在10分钟以内就利用前期项目积累的工具完成了数据整理工作，导入到软件当中，很快分析出有价值的结论，整个过程也不过半个小时而已，这让现场的学员都非常的佩服。在总结环境中，老师说**“未来的培训都要能够在现场解决问题，这样才能为客户创造最大的价值”**。由此可见，出色的数据分析能力会成为职场新人发展的助推器，正在成为职场达人。



## 1.3外面的世界



 2010年，我受上级领导的委托，负责一个专项技术攻关活动，解决数据仓库的数据质量问题，在项目期间通过和澳洲电信，挪威电信、印孚瑟的同行交流，了解到国外的主流运营商是基于数据仓库方案构建的数据仓库，其中澳洲电信的数据方案给我留下了很深的印象。

![1-ETL 方案](/assets/python-bigdata-starter/cp01/1-ETL 方案.jpg)

​             图1-1 澳洲电信的网优平台方案

本方案使用成熟中间件的作为集成方案，整个网优平台是基于多维数据中心构建的（EDN），在数据中心中核心的中间件包含三部分

1)  **ETL中间件**：   CR-X

2) **数据仓库中间件**：Oracle-10g

3）**数据挖掘中间件**： MicroStrategy

 通过以上三个中间件的协同，构成了一个稳健的多维数据中心，以此为基石，可以非常方便的构建报表分析，问题派单，投诉支撑等应用，根据澳洲电信的专家介绍，相关数据仓库的构建思路是多维数据中心为核心，基于成熟中间件来构建常规的例行需求，各种临时性的需求则通过小插件来支撑。基于这样的思路，澳洲电信重点开展了三方面的工作。

  1、首先聘请专业的咨询公司对多维数据中心的数据模型进行分析，建立**一个稳健的数据模型**。

  2、第二是建立数据管理团队对多维数据中心的运营进行保障，确保**数据采集的完备性**。

  3、第三是在网优员工中开展插件开发培训，**支撑各种临时性的小需求**。

   通过上面的分析，我们可以发现，外国运营商抓住核心信息化平台的主动权，很好兼顾长期例行需求和临时性的需求，在保障应用及时性的前提下很好的控制了费用，同时也激发了员工的创造性和主观能动性，可以非常轻松的解决数据分析与报表制作的难题，是一种非常好的构建思路。



## 1.4     选择ETL中间件

​    通过这一次向澳洲电信的学习经历，极大的开阔了我的眼界，使我认识要想解决数据分析的难题，首先要基于成熟的中间件来构建一个多维数据中心，选择一款适合的ETL中间件就显得尤为重要。

​    首先向IBM公司咨询了一下ETL中间件,IBM公司提供一个叫做DataStage的中间件，功能非常强大，但是价格非常昂贵，起步价格就在50W以上，同时后期每年需要支付一定比例的服务费用，当时开发预算非常紧张，只好放弃。接下来联系了澳大利亚墨尔本的一家公司大数据公司（<http://www.cr-x.com/>）， 也能够提供非常强大的ETL中间件，相关的产品在澳洲和美国都有广泛的应用，这家公司销售策略比较特殊，是根据ETL中间件每天处理的数据流的总量来收取授权费用，根据当时项目处理的数据量，大概需要10万元的费用，但是后续处理的数据量增加的比较快，可能在2年之内就需要追加30万元的费用，同时还涉及到和跨国公司签署合同问题，不易操作，只好再次放弃。后来，咨询了华中科技大学电信系的一位教授，他建议使用一款叫做kettle中间件，是一款绿色开源的ETL中间件。

​    2011初期，开始了基于KETTLE的实现数据仓库的数据质量核查工作。听起来很简单，起步阶段却非常困难。虽然KETTLE号称是图形化编程，但是早期的版本对调试功能支持的不是很好，很多报错都无法给出有针对性的提示，当时网上的资源也非常少，有时候一个小问题要耽误几个礼拜才能解决。大约过了4个月的时间，成功完成了全部工作，向上级领导进行了专题汇报，并投入实际的平台保障工作当中去.

   回顾整个开发过程，感觉到开源ETL中间件看起来很美，要用起来却需要花费一番苦功夫，一旦掌握了就非常方便。



## 1.5多维度数据中心

​     2013年又开发了两个小项目，有了前期的积累，开发起来也相对比较简单和轻松。 2014年为了解决空间分析的难题，构建了一个图论中间件，实现不依赖地图的空间信息分析，用于解决空间分析的问题。以前很多需要通过人工在地图上进行分析的工作，都可以通过程序运算的方式来完成，非常的轻松。这样一来就引发了大量的应用需求。

​    为了配合图论中间件的运作，考虑构建一个多维数据中心，将图论中间件需要的各类数据提前准备好。这一次，我们采用了面向对象的软件工程-一种用例驱动的设计方法（ Object oriented Software Engineering-A Use-case Driven approach）来进行项目设计。OOSE方法是瑞典著名软件科学家Ivar Jacobson博士总结的一种软件工程方法， 曾经在爱立信公司广泛进行应用，开发了多个成功的产品，例如AXE-10移动交换机。设计过程包含三个部分，首先是对多个应用人进行访谈，调研主要的工作场景和应用功能点。第二步是将多个应用功能点进行归一化分析，将这些功能点中依赖的对象进行提取，建立一个全局的领域对象模型。第三步是将领域对象模型映射为数据模型。获得了数据模型以后，就可以转入的开发工作。

​     整个开发工作大概花费了四个月的时间，中间经历很多的挫折，在大家齐心协力的努力下，顺利完成集成工作。有了图论中间件和多维数据中心的联合应用，常用的数据分析与报表制作任务都可以轻松的完成。这是才感觉到有了一点小小的成就，**不必再为数据分析和报表制作的工作量而烦恼，将更多的时间花费在通信领域的专业问题分析与研究上。**

## 1.6 选择python编程

​    有了多维数据中心以后，随着需求的增加，多维数据中心接入的数据也越来越多。此时使用kettle进行基础开发框架就日益显得力不从心了，主要存在以下三方面的问题：

1.**任务调度不灵活**。 Kettle本身不能提供强大的任务调度功能， 主要是依赖服务器的底层调度功能，运行并不稳定，也无法支持复杂的任务编排任务。

**2,****图形界面编程繁琐**：针对小规模的ETL项目，图形界面编程比较方便，但是随着数据规模的增加，图形界面编程的效率非常低。无法满足项目集成时效性的需求。

3.**部分组件需要外部集成**：由于kettle本身是侧重于数据的ETL应用，如果需要支撑一些复杂的应用处理，则需要引入基于JS，Java的组件，管理很不方便。

  为了提升多维数据中心的应用价值， 就考虑选择python作为编程工具。



## 1.7 挑战大数据中心

​    2016年下午，随着物联网应用的广泛推广，我们需要处理面向大数据的应用， 当时主流的大数据方案是基于hadoop的方案，但是这种方案对硬件的要求很高，我们的条件是不允许的。所以必须另辟蹊径，基于python来进行大数据处理，经过广泛的资料收集，借鉴了全球专家的智慧，我们终于打造了完全基于python的大数据处理方案，而且仅仅花费了很少的人工。

​    2017年以后，我也参加了不少的大数据学术会议，我发现大数据的是一个非常有价值的应用方向，但是各行各业要真正应用大数据确实非常的艰困，因为要开发一个大数据应用，需要付出非常高昂的成本，这就限制了大数据的应用场景。

​    可能是机缘巧合吧，我们从kettle出发，选择python作为核心开发框架，也探索出一条低成本的大数据集成之路。我相信各行各业都有这样的应用需求，所以我们就考虑开发一系列的课程，为大家展示一种最佳实践，一种低成本的大数据应用开发之道。

​    希望我们能作为一个桥梁，把当前国际上最先进的大数据方案，介绍给初出茅庐的你， 让你也能够成为同时掌握大数据和专业应用的复合型人才，早日成为人生的赢家，我们的开篇之作就是《python大数据入门》
